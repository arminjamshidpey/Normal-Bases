\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{mathdots}
\usepackage{esvect}

\newcommand{\osum}[2]{\alpha_{#1,#2}}
\newcommand{\osumcost}{O(n^{3/4 \omega(4/3)})}
\newcommand{\osumcosttilde}{\tilde{O}(n^{3/4 \omega(4/3)})}
\newcommand{\thecost}{\tilde{O}(\vert G \vert ^{3/4 \omega(4/3)})}

{
      \theoremstyle{acmplain}
      \newtheorem{assumption}{Assumption}
  }


% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[ISSAC 2019]{ACM Woodstock conference}{July 2019}{Beijing, China} 
\acmYear{2019}
\copyrightyear{2016}


\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Randomized Algorithms for Normal Bases}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Mark Giesbrecht
}
\affiliation{%
  \institution{Cheriton School of Computer Science
University of Waterloo}
}
\email{mwg@uwaterloo.ca}

\author{Armin Jamshidpey}
\affiliation{%
  \institution{Cheriton School of Computer Science
University of Waterloo}
}
\email{armin.jamshidpey@uwaterloo.ca}

\author{\'Eric Schost}
\affiliation{%
  \institution{Cheriton School of Computer Science
University of Waterloo}
}
\email{eschost@uwaterloo.ca}


% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{M. Giesbrecht et al.}




\begin{abstract}
It is known that for a finite Galois extension $K/F$ with $G = \mathrm{Gal}(K/F)$, there exists an element $\alpha \in K$, 
such that its orbit, $G\cdot\alpha$, forms an $F$-basis of $K$. Such an element $\alpha$, is called normal and 
$G\cdot \alpha$ is called a normal basis. In this paper we introduce a probablistic algorithm for finding a normal element 
when $G$ is either a finite abelian or metacyclic group. The algorithm is based on the fact that deciding whether a random 
element $\alpha \in K$ is normal, can be reduced to the decision problem of whether $\sum_{\sigma \in G} \sigma(\alpha)\sigma \in K[G]$ is invertible.   
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}
%
%
%\keywords{ACM proceedings, \LaTeX, text tagging}


\maketitle

\section{Introduction}

For a finite Galois extension $K/F$ with $G = \mathrm{Gal}(K/F)$, an element $\alpha \in K$
is called normal if the set of Galois conjugates of $\alpha$ forms a basis for $K$ as a
 vector space over $F$. The existence of normal element for any finite Galois extension is 
 known for a long time.
 
There is a wide range of applications of normal bases in finite fields. Fast exponentiation 
or computing the action of the Frobenius and point counting on elliptic curves are some of their applications. There are also 
applications of normal elements in characteristic zero. For a given permutation lattice and appropriate Galois extension
a normal basis is useful in computing the multiplicative invariants explicitly (see \cite{Armin} for more details).   

There are several algorithms for finding a normal element in zero characteristic and finite fields. Due to the applications of
finite fields, normal element in this case is more popular. H. W. Lenstra in \cite{LenstraNormal} introduced a deterministic 
algorithm to construct a normal element which uses $O(n^{O(1)})$ operations over the smaller field where $n$ is the degree of the
 extension. To the best of
our knowledge, the algorithm introduced by Augot and Camion (cite) is the most efficient deterministic one with cost 
$O(n^3+n^2\log q)$
where $q$ is size of the base field. Randomized algorithems for finite fileds are introduced in \cite{Giesbrecht} with cost
$O(n^2+n\log q)$ and \cite{Kaltofen} with cost $O(n^{1.8})$. In characteristic zero, A. Poli gave an algorithm for abelian extensions in \cite{Poli} with $O(n^{O(1)})$ and Schlickewei and Stepanov introduced an algorithm in the cyclic case with
$O(n^{O(1)})$ as the computation cost \cite{Stepanov}. Girstmair has an algorithm which uses $O(n^4)$ operations over the base field to construct a normal element for a general finite Galois extension in characteristic zero \cite{Girstmair}. 

In this paper we will introduce a randomized algorithm for finding a normal element in case of abelian and metacyclic extensions
 which is subquadratic in the degree of the extension. The idea behind the algorithm is similar to ideas of 
\cite{Giesbrecht,Kaltofen}. Since a part of the assumptions in the results of this paper is the same, we state it here 
for future references .

\begin{assumption}\label{assum}
$K/F$ is a finite Galois extension given by $ F[x]/f$ for an irreducible polynomial $f\in F[x]$ of degree $n$, with
 $G = \mathrm{Gal}(K/F)$. Moreover $\alpha$ is an element of $K$ and the action of generators of $ G$, such as $g$, on $K$ is given by its action on $\bar{x} = x \mod f.$ In other words, $g(\bar{x})$ is given for any generator $g$ of $G$.
\end{assumption}

under the Assumption \ref{assum} we choose a random element $\alpha$ of $K$. We use a known fact that, $\alpha$ is normal 
if $M_G(\alpha) \in M_{n\times n}(K)$, an associated matrix to $\alpha$ (see Section 2 for the definition), is invertible. 
Afterwards we reduce the invertiblity of $M_G(\alpha)$ to invertiblity of a random projection an associated element 
$\osum{G}{K} \in K[G]$ which lies in $F[G]$, the group ring of $G$ over $F$. 

Section \ref{sec:pre} of this paper is devoted to provide the definitions and preliminary discussions. In Section \ref{sec:osum} 
the orbit sum problem is discussed and two algorithms are presented which can be applied to compute projections of orbit sums.
Finally in the last section we do what?!


\section{Preliminaries} \label{sec:pre}
One of the well-known proofs of the existence of a normal element for a finite Galois extension, suggests a randomized 
algorithm for finding such an element. We state a part of such proof up to the point that we need (see \cite[Theorem 6.13.1]{Lang} for the complete proof). Assume $K/F$ is a finite Galois extension with Galois group $G = \lbrace g_1 , \ldots ,
 g_n \rbrace$. If $x \in K$ is a normal element, then
 \begin{equation}\label{eq:fstrow}
 \sum_{j=1}^n 
 c_j g_j(x)=0, \,\,\, c_j \in F 
 \end{equation} 
 implies $(c_1, \ldots ,c_n) = 0$. For each $i \in \lbrace 1, \ldots , n\rbrace$, applying $g_i$ to equation (\ref{eq:fstrow}) yields
\begin{equation} \label{eq:otherrow}
 \sum_{j=1}^n 
 c_j g_i g_j(x)=0.
 \end{equation}
 Using equations \ref{eq:fstrow} and \ref{eq:otherrow} one can form a linear system $M_G(\alpha)\textbf{v} = \textbf{0}$ where 
 $$ M_G(x) =
\begin{bmatrix}
g_1 g_1(x) & g_1 g_2(x) & \cdots & g_1 g_n(x) \\
g_2 g_1(x) & g_2 g_2(x) & \cdots & g_2 g_n(x) \\
\vdots		& \vdots	& \vdots & \vdots \\
g_n g_1(x) & g_n g_2(x) & \cdots & g_n g_n(x) \\
\end{bmatrix}. 
 $$
 Equation \ref{eq:otherrow} shows that $M_G(x)$ is non-singular. The rest of the proof shows that there exists $\alpha \in K$ with $\det(M_G(\alpha))\neq 0$.
 
 
 Above discussion can be used as the basic idea of a randomized algorithm for finding a
 normal element.\\
 \\
 \textbf{Algorithm 1.} \label{alg:naive}
 The algorithm takes a finite Galois extension $K/F$ with 
 $G =  \mathrm{Gal}(K/F) = \lbrace g_1, g_2, \ldots , g_n \rbrace$ and returns a normal
 element $\alpha \in K$.
 \begin{description}
 \item \textbf{step 1.} choose a random element $\alpha$ in $K$.
 \item \textbf{step 2.} write the matrix $M_G(\alpha)$.
 \item \textbf{step 3.} if $ M_G(\alpha)$ is invertible\\
% \hspace{2cm} if $r = n$ \\
 \hspace{10cm} return $\alpha$\\
 \hspace{2cm} else \\
 \hspace{5cm} go to step 1.\\  
 \end{description}
 
 Clearly the main concerns are step 2 and step 3. As a naive way, one can compute all the entries of the matrix and then 
 use linear algebra to compute the determinant of $M_G(x)$ which uses $O(n^3)$ operations. However, since this is not efficient
 enough, we want to avoid writing the matrix and computing determinants. Before explaining how we can check the invertiblity 
 in an efficient way,it is worth talking about the probability of success for a random choice of $\alpha$.
 
 Since we are working over characteristic zero, we are able to consider a finite subset
 of $K \cong F^n$ such that the probability of failure is $\dfrac{1}{n}$, using 
 Schwartz-Zippel lemma (Proposition \ref{thm:zippel}).
 
 If $\lbrace a_1, \ldots , a_n \rbrace$ is an $F$-basis for $K$, then in Equations
   \ref{eq:fstrow} and \ref{eq:otherrow}, $x$ can be written as $\sum_{i = 1}^nx_i
    a_i$. Now we can rewrite 
    $$
M_G(x) = M_G(x_1,\ldots,x_n) =  $$
$$
\begin{bmatrix}
\sum_{i = 1}^n g_1 g_1(a_i)x_i & \sum_{i = 1}^n g_1 g_2(a_i)x_i & \cdots & 
\sum_{i = 1}^n g_1 g_n(a_i)x_i \\
\sum_{i = 1}^n g_2 g_1(a_i)x_i & \sum_{i = 1}^n g_2 g_2(a_i)x_i & \cdots & 
\sum_{i = 1}^n g_2 g_n(a_i)x_i \\
\vdots		& \vdots	& \vdots & \vdots \\
\sum_{i = 1}^n g_n g_1(a_i)x_i & \sum_{i = 1}^n g_n g_2(a_i)x_i & \cdots & 
\sum_{i = 1}^n g_n g_n(a_i)x_i \\
\end{bmatrix}    
    $$ 
 The following theorem enables us to talk about probability of success (or failure).
 \begin{proposition}\cite[Proposition 98]{Zippel} \label{thm:zippel}
Let $P \in A[X_1, \ldots, X_n]$ be a polynomial with total degree $D$ over an integral domain $A$. Let $S$ be a subset of $A$ of cardinality $B$. Then $$Pr(P(x_1, \ldots , x_n)=0:x_i \in S) \leq \dfrac{D}{B}.$$
\end{proposition}

Now $\det(M_G(x)) \in K[x_1, \ldots , x_n]$ is a polynomial of total degree $n$ and $K$ is a field. If $char(K) =0$, then by considering $S \subset F \subset K$ where $|S| = n^2$ and applying the above proposition we get $$Pr(\det(M_G(r_1,\ldots , r_n)) = 0 : r_i \in S)\leq \dfrac{n}{n^2}= \dfrac{1}{n}.$$
 
Now let us take a look at steps 2 and 3 in Algorithm \ref{alg:naive}. Since in the cyclic case $M_G(\alpha)$ is circulant,
 in \cite{Giesbrecht} instead of writing the matrix, the invertibility test is done by means of finding the gcd of an 
 associated polynomial 
$$P(x) = \sum_{i = 1}^n g_ig_1(\alpha)x^{i-1}$$ 
 to $M_G(\alpha)$ and $x^n-1$. Note that in this case the group ring of $G$ over $K$, $K[G]$ is isomorphic to $K[x]/(x^n-1)$,
 $P(x)$ is the isomorphic copy of the orbit sum of $\alpha$ over $K[G]$, namely
 $$\alpha_{G,K} = \sum_{g \in G} g(\alpha)g \in K[G].$$
 Moreover, invertibility of $\osum{G}{K}\in K[G]$ is equivalent of having $\gcd (P(x),x^n-1) =1.$
 
 A close look at $M_G(x)$ tells us it is exactly the matrix of (left) multiplication by $$\osum{G}{K} \in K[G].$$  This gives an idea to modify Algorithm \ref{Alg:Naive}. Instead of writing $M_G(x)$ and test its  invertiblity, 
 we can write $\osum{G}{K}$ and test if it is invertible in $K[G]$. Although testing the invertibility of $\osum{G}{K}$ might be efficient in
 comparison to computing the determinant of a matrix in $K$, we prefer to do the computations over $F$ rather than $K$. The  following lemma
 comes handy to pass the computations from $K$ to $F$.


\begin{lemma}\label{Lem:Proj}
Assume $\alpha \in K$. $M_G(\alpha) \in M_{n \times n}(K)$ 
is invertible if and only if $$l(M_G(\alpha)) =  [l(g_ig_j(\alpha))]_{ij}  \in M_{n \times n}(F)$$ is invertible,
 where $l$ is a generic projection of $K$ to $F$.
\end{lemma}

\begin{proof}
$(\Rightarrow)$ Since $K = F(\theta)$, for a fixed $\alpha$, any entry of $M_G(\alpha)$ can be written as 
\begin{equation}\label{Eq:PrimElm}
\sum_{k= 0}^{n-1} a_{ijk}\theta^k
\end{equation}
 and the corresponding entry in $l(M_G(\alpha))$ (for a random projection $l$)
 can be written $\sum_{k= 0}^{n-1} a_{ijk}l_k$ with $l_k\in F$. If we replace this specific choice of $l_k$'s by 
 indeterminates $x_k$'s, we can see $\det(x(M_G(\alpha))$ is a polynomial in $F[x_1, \ldots, x_n].$ Let 
 $$P(x_1, \ldots, x_n) = \det(x(M_G(\alpha)).$$ 
 Considering $P \in K[x_1, \ldots , x_n]$, one can verify that 
 \begin{equation}\label{Eq:Det}
 det(M_G(\alpha))= P(1, \theta, \ldots, \theta^{n-1}) \neq 0
 \end{equation}
 since $M_G(\alpha)$ is invertible. Equation \ref{Eq:Det} implies that $P(x_1, \ldots, x_n)$ is not identically zero over $F$. Hence applying Theorem \ref{Thm:Zippel} with appropriate choice of parameters, we can see 
 the projection of $M_G(\alpha)$ is invertible for a generic choice of projection. 
 
 $(\Leftarrow)$  Note that elements of $G$ can act on 
 rows of $M_G(\alpha)$ entrywise and the action permutes the rows of $M_G(\alpha)$. Assume $\varphi : G \longrightarrow \mathfrak{S}_n$ is the group homomorphism 
 such that $g(M_i) = M_{\varphi(g)(i)}$ where $M_i$ is the $i$-th row of $M_G(\alpha)$.
 
 Assume $M_G(\alpha)$ is not invertible. Following the proof of \cite[Lemma 4]{Armin}, we show that there exists a non-zero $\textbf{u} \in F^n$ in the kernel of $M_G(\alpha)$. 
 
 Since $M_G(\alpha)$ is singular, there exists a non-zero $\textbf{v} \in K^n$  such that $M_G(\alpha)\textbf{v} = 0$ and $\textbf{v}$ has the minimum number of non-zero entries. Let $i \in  \lbrace 1, \ldots , n \rbrace$ such that $v_i \neq 0$. Define $\textbf{u} = \dfrac{1}{v_i}\textbf{v}$. It is clear  that $M_G(\alpha)\textbf{u} = 0$ which means $M_j \textbf{u} = 0 $ for $j \in \lbrace 1, \ldots, n \rbrace$. For $g \in G$
 \begin{equation}
  g(M_j \textbf{u}) = M_{\varphi(g)(j)} \textbf{u}= 0
 \end{equation}
 Since the above equation holds for any $j$ we conclude that $$M_G(\alpha)g(\textbf{u})= 0$$ hence
 $g(\textbf{u})-\textbf{u}$ is in the kernel of $M_G(\alpha)$. On the other hand since the $i$-th entry 
 of $\textbf{u}$ is one, the $i$-th entry of $g(\textbf{u}) -\textbf{u}$ is zero. Thus the minimality assumption
 on $\textbf{v}$ shows that $g(\textbf{u}) -\textbf{u} = 0$ and equivalently $g(\textbf{u})=\textbf{u}$. This 
 means $\textbf{u} \in F^n$.
 
 
 Now we show that $l(M_G(\alpha))$ is not invertible for all
 choices of $l$. By Equation \ref{Eq:PrimElm} we can write 
 $$M_G(\alpha) = \sum_{j = 1}^n M^{(j)} \theta^j$$ 
 where $M^{j} \in M_{n \times n}(F)$. 
 
 Now $M_G(\alpha) \textbf{u} =0$ yields $M^{(j)}\textbf{u} = 0$ for $j \in \lbrace 1, \ldots , n \rbrace$. Hence
 $$\sum_{j = 1}^n M^{(j)} l_j \textbf{u} = 0$$ for any choice of $l_j$'s in $F$. So $l(M_G(\alpha))$ is not invertible for any choice of $l$.
\end{proof} 
Lemma \ref{Lem:Proj} enables us to test invertibility of a random projection of $l(u_{G,K}) $ i.e. $\sum_{g \in G}
 l(g(\alpha))g \in F[G]$. Although we can avoid writing $M_G(\alpha)$, we still need to compute $l(u_{G,K})$ which
 we call it the orbit sum projection problem.


\section{Computing projections of the orbit sum}\label{sec:osum}

So far we have reduced the problem of invertiblity of $M_G(\alpha)$ to $l(\osum{G}{K}) \in F[G]$ for a generic
projection $l$. In order to compute $l(\osum{G}{K})$ we need $l(g(\alpha)) \in F$ for $g \in G$. In \cite{Kaltofen} the problem of
computing $l(g(\alpha))$'s for all $g \in G$ is called automorphism projection problem. In loc. cit. a transformation of this problem
is defined as the automorphism evaluation problem. For a given $P = \sum_{g \in G}a_g g \in K[G]$ the automorphism evaluation problem
is computing $P(\alpha) = \sum_{g \in G}a_g g(\alpha)$. Moreover in \cite{Kaltofen} it is proven that the above problems are
 computationally equivalent. Hence instead of solving the automorphism projection problem directly, we can solve the automorphism
 evaluation problem.

In this section we present algorithms to compute $\osum{G}{K}$ in case that $G$ is either abelian or metacyclic. 
Before presenting the algorithm for the mentioned groups we need to provide some tools. Afterwards we explain an algorithm
for a cyclic extension which is similar to the algorithm introduced in \cite{Kaltofen}. Then we present a generalized version 
of the algorithm for cyclic case, which soves the problem for an abelian extension. Finally based on the ideas for cyclic and 
abelian cases we introduce an algorithm which solves the problem for any group ehich can be presented as 
$$\lbrace a^ib^j: 0 \leq i \leq n,  0\leq j \leq m, n \leq m \rbrace.$$ 
As an specific case the mentioned algorithm works for metacyclic groups.

The following lemmas are variants of \cite[Lemma 3 $\&$ Lemma 4]{Kaltofen}, with a slight modification in the proofs.

\begin{lemma}\cite{Kaltofen}\label{modcom}
Assume $K$ is a field, $f\in K[x]$ is of degree $n$ and $s = \lceil\sqrt{n}\rceil$. Given $g_1, \ldots , g_{s}$ and 
$h \in K[x]$ of degree less than $n$, $$g_1(h), \ldots g_{s}(h) \mod f$$ can be computed in
$O(n^{\frac{3}{4}\omega(\frac{4}{3})})$ where $\omega(\frac{4}{3})$ is the exponent of rectangular matrix 
multiplication as introduced in \cite{LeGall}. 
\end{lemma}

\begin{proof}
Let $t = \lceil n^{3/4} \rceil$ and rewrite $g_1 , \ldots , g_s$ as 
$$g_i = \sum_{0 \leq j < n/t} g_{ij}x^{tj}.$$
Now $g_{ij}$'s are polynomials of degree less than $t$. The next step is to compute $H_i = h^i \mod f$ for $i = 0 , \ldots , t$.
Having $H_i$'s in hand one can form the matrix $H = \left[ H_1 \vert \cdots \vert H_t \right]^T$ where each column is the vector of 
the element $H_i$ (with coefficients in $K$) so the matrix $H$ is of size $t \times n$. We form 
$$A = \left[\bar{g}_{10}\vert \cdots \vert \bar{g}_{1(n/t-1)}\vert \cdots \vert \bar{g}_{s0}\vert \cdots \vert \bar{g}_{s(n/t-1)}\right]^T,$$
where $\bar{g}_{ij}$ is the  vector of $g_{ij}$. In order to compute $g_{ij}(h)$ one can compute $A \cdot H$. Using 
results from \cite{LeGall}, this can be done in $O(n^{3/4 \omega(4/3)})$. The last step to get $g_i(h)$, is to substitute $H_t$ 
for $x^t$ and performing a Horner evaluation scheme. The dominant term in the cost of this calculation, is the cost of computing $AH$ which can be carried out using $O(n^{3/4 \omega(4/3)})$ operations in $K$.
\end{proof}

\begin{lemma}\cite{Kaltofen}\label{lem:selfcomp}
Assume $K = F[x]/f$ is a finite Galois extension of $F$, for an irreducible $f\in F[x]$ of degree $n = n_1 \cdots n_r$ and $s_i = \lceil\sqrt{n_i}\rceil$. Given $g_1, \ldots , g_{r} \in G = \mathrm{Gal}(K/F)$ (with $\mathrm{ord}(g_i) = n_i$) with their actions on 
$\bar{x}=x \mod f$ and $\alpha \in K$,
$$g_1^{i_1}\cdots g_r^{i_r}(\alpha) , 1 \leq j \leq r, 
0 \leq i_j \leq s_i$$ can be 
computed in $\osumcosttilde$ where $\omega(\frac{4}{3})$ is the exponent of rectangular matrix 
multiplication as introduced in \cite{LeGall}. 
\end{lemma}

\begin{proof}
% At first assume we have computed 
%$g_1(x), \ldots , g_1^m(x) \mod f$. Then we can compute $g_1^{m+1}(x), \ldots , g_1^{2m}(x)$ by
%computing $$g_1^{m+1}(g_1(x)), \ldots , g_1^{m+1}(g_1^m(x)) \mod f$$ using Lemma \ref{modcom}. Hence by repeating the doubling method 
%$\log s_1$ times, we can compute $g_1(x), \ldots , g_1^{s_1}(x) \mod f$ in $O(\log s_1) \times \osumcost$. Now if we have computed
Suppose $\alpha = \sum_{0\leq i \leq n-1} a_i \bar{x}^i$. For any $g_j$ we have 
\begin{equation}\label{eq:comute}
g_j^t(\alpha) = \sum_{0\leq i \leq n-1} a_i g_j^t(\bar{x}^i) = \alpha(g_j^t(\bar{x}))
\end{equation}
since $g_j$ acts trivially on $a_i$. Now assume we have computed 
 $$g_1(\alpha), \ldots , g_1^m(\alpha).$$ Then we can compute  $g_1^{m+1}(\alpha), \ldots , g_1^{2m}(\alpha)$ by computing $$g_1^m(g_1(\alpha)), \ldots , g_1^m(g_1^m(\alpha)) $$ using Lemma \ref{modcom} and Equation \ref{eq:comute}. By applying the above 
 doubling method, the computation of $g_1(\alpha), \ldots , g_1^{s_1}(\alpha)$ can be done in $ O(\log s_1) \times \osumcost$.
 Note that here we need to compute the action of (even) powers of $g_1$ on $\bar{x}$ as well. However, it is just a one time
 computation and does not change the complexity. 

The next step is to apply $g_2^i$ to $g_1(\alpha), \ldots , g_1^{s_1}(\alpha)$ for $0\leq i \leq s_2$. Again 
$g_1^j(\alpha) = \sum_{i} a_{ij} \bar{x}^i$ and $$g_2^t(g_1^j(\alpha)) = \sum_{i} a_{ij} (g_2^t(\bar{x}))^i = g_1^j(\alpha)(g_2^t(\bar{x})).$$
Using the above fact we compute 
$$g_2(g_1(\alpha)), \ldots , g_2(g_1^{s_1}(\alpha))$$
by applying Lemma \ref{modcom} which costs $\osumcost$. At this point we have 
$$\begin{array}{lll} g_1(\alpha)& \ldots & g_1^{s_1}(\alpha)\\ g_2(g_1(\alpha))& \ldots & g_2(g_1^{s_1}(\alpha))\end{array}$$
and by applying $g_2^2$ (using the above method) we get 
$$\begin{array}{lll} g_1(\alpha)& \ldots & g_1^{s_1}(\alpha)\\ g_2(g_1(\alpha))& \ldots & g_2(g_1^{s_1}(\alpha))\\
g_2^2(g_1(\alpha))& \ldots & g_2^2(g_1^{s_1}(\alpha))\\g_2^3(g_1(\alpha))& \ldots & g_2^3(g_1^{s_1}(\alpha))
\end{array}$$
then by repeating the above doubling method we get
$$g_2^{i_2}(g_1^{i_1}(\alpha)), \, 0\leq i_1\leq s_1, 0 \leq i_2 \leq s_2,$$
using $O(\log s_2)\cdot \osumcost + O(\log s_1)\cdot \osumcost$ operations in $F$ (in total), which is equal to $O(\log (s_1s_2))\osumcost$. 

Doing the same for $g_3 , \ldots , g_r$ we see the cost of the computation is 
$$O(\log (s_1\cdots s_r))\osumcost = \osumcosttilde.$$

Note that for each $g_i$ we have to compute $g_i^j$ for $0 \leq j \leq s_i$ but this is just a one time computation for each $g_i$
which does not change the total cost.
\end{proof}

%\begin{lemma}\label{lem:transmodcomp}
%Assume $K = F[x]/f = F[\bar{x}]$ is a cyclic extension of $F$ where $f$ is of degree $n$, and $g \in \mathrm{Gal}(K/F)$  
% and $L: K\rightarrow F$ is a linear projection. we can compute
%$L \circ g^j$ for all $0 \leq j < \sqrt{n}$ using $\osumcosttilde$ operations in $F$. 
%\end{lemma}
%
%\begin{proof}
%for a fixed $j$, $L \circ g^j$ is determined by its image on $\bar{x}$. So our goal is to compute $T_j = L \circ g^j(\bar{x})
% = s^j\cdot L$ where $s = g(\bar{x})$.
% 
%If we have $T_1, \ldots , T_m$, then we can compute $T_{m+1}, \ldots, T_{2m}$ in the following way.
%At first we compute $s_1 , \ldots, s_{\lceil n^{3/4} \rceil}$ and form the matrix 
%$S = \begin{bmatrix}
%s^0 & \vert & \cdots & \vert & s_{\lceil n^{3/4} \rceil}^t
%\end{bmatrix}
%$
%which is of size $\lceil n^{3/4} \rceil \times n$. Then we form the matrix
%$$T = 
%\left[ \begin{array}{c|c|c|c|c|c|c}
%s^0T_1 & \cdots & s^{\lceil n^{1/4} \rceil -1} & \cdots &  s^0T_m &  \cdots & s^{\lceil n^{1/4} \rceil -1} T_m
%\end{array} \right]
%$$
%which is of the size $n \times \lceil n^{3/4} \rceil$. finally we compute $S \cdot T$ using rectangular matrix multiplication 
%result of \citep{LeGall} which gives the desired complexity.
%\end{proof} 


Now we have enough tools for computing the automorphism projection in the cyclic case where $G = \langle g: g^n =1 \rangle$
Which is basically \cite[Algorithm AP]{Kaltofen}. 
Assume $L = \begin{bmatrix}
l_1 & l_2 & \ldots & l_n
\end{bmatrix}$ is given where $l_i \in F$. The goal is to compute 
\begin{equation}\label{eq:autproj}
L(g^i(\alpha)) = (L \circ g^{j \lceil \sqrt{n} \rceil})\cdot (g^i(\alpha)), 0 \leq j < \lceil \sqrt{n} \rceil, 0 \leq i \leq \lceil \sqrt{n} \rceil -1, 
\end{equation}
for a given $\alpha \in K$. 
%For a given polynomials $h,f \in F[x]$ and projection $L$, computing $L(h^i \mod f)$ for
%$0\leq i \leq r$ is called transposed modular composition. In order to do such computation in \cite{Shoup} has defined a subtask which is called transposed modular multiplication and 
Let $T_j = L\circ g^{j \lceil \sqrt{n} \rceil}$.
%and this operation is called  note that in \cite{Shoup} . 
This computation can be done in 3 steps:

\textbf{step 1.} compute $g^i(\alpha)$ for $i = 1, \ldots , \lceil \sqrt{n} \rceil$. Using Lemma \ref{lem:selfcomp} this 
can be carried out by doing $\osumcosttilde$ operations in $F$.

\textbf{step 2.} If we have $T_1, \ldots , T_m$, then we can get $T_{m+1}, \ldots T_{2m}$ by computing $T_i \circ g^{m\sqrt{n}}$
which is equivalent of computing 
\begin{equation}\label{eq:projdoubling}
\tilde{T_i} = \begin{bmatrix} s^0 \cdot T_i & \cdots & s^{n-1}\cdot T_i \end{bmatrix}
\end{equation}
where $s_m = g^{m\sqrt{n}}(\bar{x})$. In order to compute row vector presented in \ref{eq:projdoubling} for a fixed $i$, let 
$r = n^{1/4}$ and compute 
$$
\bar{T_i} = \left[\begin{array}{c|c|c}
s_m^0T_1 & \cdots & s_m^{r-1}T_i
\end{array} \right]
$$
and multiply it by 
$$S = \left[
\begin{array}{c}
s_m^0\\
\hline
s_m^{r}\\
\hline
s_m^{2r}\\
\hline
\vdots\\
\hline
s_m^{(n^{3/4}-1)r}
\end{array} \right].
$$
Note that in the above computation one can use the algorithm given in \cite{Shoup}
for transposed modular composition. 
One can see $\tilde{T_i} = S \cdot \bar{T_i}$ and $T_{m+1}, \ldots, T_{2m}$ can be computed by doing
$$ S \cdot \begin{bmatrix} \bar{T_1}\vline & \cdots \vline & \bar{T_m} \end{bmatrix}$$
where the second matrix is of size $n \times m$. This completes the doubling method. In the last step of the doubling method
$m = \sqrt{n}/2$ so the matrix product can be computed using $\osumcost$ operations in $F$ which gives the cost $\osumcosttilde$
for this step.

%\textbf{step 2.} compute $n/\lceil \sqrt{n} \rceil$ elements $h_i = \sum_{j = 1}^ {\lceil \sqrt{n} \rceil-1}c_{ij}g^{j}(\alpha)$ which is actually a rectangular matrix multiplication 
%\begin{equation}
%\left(\begin{array}{llll}
%c_{00} & c_{01} & \cdots & c_{0\lceil \sqrt{n} \rceil -1}\\
%c_{10} & c_{11} & \cdots & c_{1\lceil \sqrt{n} \rceil -1}\\
%\vdots & \vdots & \vdots & \vdots \\
%c_{00} & c_{01} & \cdots & c_{0\lceil \sqrt{n} \rceil -1}\\
%\end{array}\right)\cdot 
%\left(\begin{array}{l}
%\vv{g^0(\alpha)}\\
%\vv{g^1(\alpha)}\\
%\vdots\\
%\vv{g^{\lceil \sqrt{n} \rceil -1}(\alpha)}
%\end{array}\right),
%\end{equation}
%where $\vv{g^i(\alpha)} \in F^n$ is the vector of coefficients of $g^i(\alpha)$ in $F$. The multiplication can be 
%carried out using $\osumcost$ operations in $F$.

\textbf{step 3.} So far we have computed $T_j$'s and $g^i(\alpha)$'s. The result can be acheived by doing the following 
rectangular matrix multiplication:

$$
\begin{bmatrix}
T_0 & \vert & \cdots & \vert &T_{\lceil \sqrt{n} \rceil -1}
\end{bmatrix}^T \cdot
\begin{bmatrix}
\vv{g^0(\alpha)} & \vert & \cdots & \vert & \vv{g^{\lceil \sqrt{n} \rceil -1}}
\end{bmatrix}.
$$
Since the left matrix is of size $\lceil \sqrt{n} \rceil \times n$ and the right matrix is of size $n \times \lceil \sqrt{n} \rceil$,
using results from \cite{LeGall} this can be done by performing $O(n^{1/2\omega(2)})$ operations in $F$.

%\textbf{step 3.} Finally we want $\sum_{i = 0}^{n/\lceil \sqrt{n} \rceil} g^{i\lceil \sqrt{n} \rceil}(h_i)$. Let
%$\bar{g} = g^{\lceil \sqrt{n} \rceil}$ which is computed in step 1 already. We apply powers of $\bar{g}$ $h_i$'s in
%the following way: 
%\begin{footnotesize}
%$$
%\begin{array}{lllllllll}
%&h_0 & h_1 & h_2 & h_3 & h_4 & h_5 & h_6 &  \cdots \\
%\bar{g} \rightarrow &	& \downarrow & & \downarrow & & \downarrow & &  \cdots \\
%&h_0 & g(h_1) & h_2 & g(h_3) & h_4 & g(h_5) & h_6 &  \cdots \\	
%\bar{g}^2 \rightarrow &	&  & \downarrow& \downarrow & &  &\downarrow &  \cdots \\
%&h_0 & g(h_1) & g^2(h_2) & g^3(h_3) & h_4 & g(h_5) & g^2(h_6) & \cdots \\	
%\bar{g}^4 \rightarrow&	&  & &  & \downarrow& \downarrow &\downarrow & \cdots \\
%&h_0 & g(h_1) & g^2(h_2) & g^3(h_3) & g^4(h_4) & g^5(h_5) & g^6(h_6) &  \cdots \\		
%\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots 
%\end{array}
%$$
%\end{footnotesize}
%This means we do $O(\log \sqrt{n})$ times $\osumcost$ or equivalently $\osumcosttilde$ operations in $F$. 

In the next section
we present a generalized version of the above algorithm which works for abelian extensions.

\subsection{Abelian Groups}
Assume $G$ is an abelian group presented as 
$$ \langle g_1, \ldots , g_n: g_{1}^{e_1} = \cdots = g_{n}^{e_n} = 1 \rangle$$
 where $ e_i \in \mathbb{N}$
is the order of $g_i$. Our goal is to compute $S = (P(g_1,  \ldots, g_n))(\alpha)$ where $P(x_1, \ldots,x_n) \in F[x_1, \dots , x_n].$
We can rewrite $S$ as 
$$\sum_{\substack{0 \leq i_1 < e_1/t_1 \\ \vdots \\ 0 \leq i_n < e_n/t_n}} \hat{g}_1^{i_1} \cdots \hat{g}_n^{i_n}(\sum_{\substack{0 \leq j_1 \leq t_1-1\\ \vdots \\ 0 \leq j_n \leq t_n-1}} a^{i_1, \ldots , i_n}_{j_1, \ldots , j_n}g_1^{j_1}\cdots g_n^{j_n}(\alpha))$$
where $t_i = \lceil \sqrt{e_i}\rceil$ and $\hat{g_i} = g_i^{t_i}.$

It is simpler to think of monomials in $g_i$'s as integral points of $\mathbb{R}^n$ space, i.e.
$(a_1, \ldots , a_n)$ represents $g_1^{a_1}, \ldots g_n^{a_n}$. If this is the case then the above equation suggests to 
partition the $n$-cell $$[0,e_1] \times \cdots \times [0, e_n]$$ into $n$-cells of the same size as 
$$[0,\lceil \sqrt{e_1} \rceil] \times \cdots \times [0, \lceil \sqrt{e_n} \rceil],$$ 
then we can start to compute the automrphism evaluations in the $n$-cell containing the origin. After that each time we apply one 
of the generators we are moving to another $n$-cell.

We compute the projection of the orbit sum of $\alpha$ in 3 steps.

\textbf{step 1. Compute the actions of the points in the $n$-cell containing the origin on $\alpha$}. At first we compute 
$g_1(\alpha), \ldots , g_1^{t_1}(\alpha)$ in $\tilde{O}(\sqrt{\vert G \vert}^{\omega({\frac{3}{4}})})$ operations in $F$, by lemma 4. Now
we apply 

 $g_2$ to $g_1(\alpha), \ldots , g_1^{t_1}(\alpha)$ then after applying $g_2^2$ to
\[
 \begin{array}{lll}
 g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
 g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) 
\end{array} 
\]
we have computed
\[
 \begin{array}{lll}
 g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
 g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) \\
 g_2^2(g_1(\alpha))& \ldots & g_2^2(g_1^{t_1}(\alpha)) \\
 g_2^3(g_1(\alpha))& \ldots & g_2^3(g_1^{t_1}(\alpha))
\end{array} 
\]
If we continue by applying $g_2^4, \ldots$ we are able to get
\[
 \begin{array}{lll}
 g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
 g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) \\
 \vdots & \vdots & \vdots\\
 g_2^{t_2}(g_1(\alpha))& \ldots & g_2^{t_2}(g_1^{t_1}(\alpha)) \\
\end{array} 
\]
Note that for computing 
$$g_2^{j}(g_1(\alpha)), \ldots , g_2^{j}(g_1^{t_1}(\alpha))$$
we use the similar idea which is used in the proof of Lemma \ref{lem:selfcomp}.
This shows that above computation can be done in $(\log \sqrt{t_2})(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$ operations in $F$. By doing similar operations for $g_3, \ldots, g_n$ on all the outputs we can compute the corresponding images to the elements
of the $n$-cell containing the origin in $\tilde{O}(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$

\textbf{step2. computing automorphism evaluations in the $n$-cell containing the origin.}

We want to compute
$$T_{i_1, \ldots, i_n} = \sum_{\substack{0 \leq j_1 \leq t_1\\ \vdots \\ 0 \leq j_n \leq t_n}} a^{i_1, \ldots , i_n}_{j_1, \ldots , j_n}g_1^{j_1}\cdots g_n^{j_n}(\alpha)$$
for $0 \leq i_1-1 \leq t_1, \ldots , 0 \leq i_n \leq t_n-1$ which can be done by a rectangular matrix multiplication with cost
$\tilde{O}(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$.

\textbf{step 3. applying $\hat{g}_i$'s.}

We have already computed $\hat{g}_i$ in step 1. Similar to step 1, we can apply $g_i^s$ 
for $s = 1, 2, 4 , \ldots, t_1$ to $T_{i_1, \ldots , i_n}$ which can be done using $(\log t_i)(\vert G \vert^{\frac{1}{2}\omega
(\frac{4}{3})})$ operations in $F$. Thus this step can be carried out by $\tilde{O}(\vert G \vert^{\frac{1}{2}\omega
(\frac{4}{3})})$ operations in $F$. 

\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is an abelian group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

\subsection{Metacyclic Groups}

A Group $G$ is called metacyclic if it has a normal cyclic subgroup, $H$, such that $G/H$ is cyclic. It is known that any group
with a square free order, is metacyclic and elements of a metacyclic group can be presented as 
\begin{equation}\label{eq:metacyclic}
\langle \sigma,\tau: \sigma^n = 1, \tau^{-1}\sigma \tau = \sigma^r, \tau ^m = \sigma^s \rangle
\end{equation}
where $m,n,r,s \in \mathbb{N}, r,s \leq n,$ and $r^m = 1 \mod n , rs = s \mod n$. Moreover we know that all element of a metacyclic
 group can be presented by $$\sigma^i \tau^j, \,\,\, 0\leq i \leq m-1, 0\leq j \leq n-1,$$ 
see \cite[P.88, Proposition 1]{Johnson}, \cite[P.334]{Curtis} for more details. Dihedral group 
$$D_{2n} = \langle \sigma,\tau: \sigma^n =\tau^2 = 1, \sigma \tau = \tau \sigma^{-1} \rangle, $$
is an example of metacyclic groups. another well-known metacyclic group is generalizewd quaternion
 group which can be presented as
 $$Q_n = \langle \sigma,\tau: \sigma^n =\tau^2, \tau \sigma \tau^{-1} = \sigma^{-1} \rangle.$$
 We know that elements of $Q_n$ are of the form 
 $$\sigma^i\tau^j, 0 \leq i \leq 2n-1 , 0\leq j \leq 1.$$
 
 Assume $G$ is given by Equation \ref{eq:metacyclic}, Assumption \ref{assum} holds and 
 $$P(x,y) = \sum_{i= 0}^n \sum_{j=0}^m c_{ij}x^iy^j = \sum_{i= 0}^n x^i\left(\sum_{j=0}^m c_{ij}y^j \right) \in F[x,y]. $$
 We want to get $(P(\sigma,\tau))(\alpha)$. Let $H = \langle x \rangle$ is a normal subgroup of $G$ such that $G/H = \langle yH
  \rangle$. Since $yxy^{-1}$ belong to $H$, elements in $G$ also be presented as $b^i a^j$. Hence 
 without loss of generality we may assume $n \leq m$. Assume $s = \lceil \sqrt{mn} \rceil$ and
 $$H_i = (\sum_{j=0}^m c_{ij}\tau^j)(\alpha) = \sum_{j=0}^{m/ s} \tau^{js}\left(\sum_{t= 0}^{s}c'_{it}\tau^j(\alpha)\right).$$
 Suppose $T_{ij} = \sum_{t= 0}^{s}c'_{it}\tau^j(\alpha)$. In order to compute each $T_{ij}$ we start by computing 
 $$\tau(\alpha), \tau^2(\alpha), \ldots , \tau^{s}(\alpha).$$
 By Lemma \ref{lem:selfcomp} this can be done using $\tilde{O}((mn)^{3/4 \omega(4/3)}$ operations in $F$. Then to get $T_{it}$, we compute
 $$
\left(\begin{array}{lll}
c'_{00} & \cdots & c'_{0s}\\
\vdots & \vdots & \vdots\\
c_{m/s0} & \cdots & c_{m/ss}\\
\vdots & \vdots & \vdots\\
c_{n0} & \cdots & c_{ns}
\end{array} \right)
\cdot
\left( \begin{array}{l}
\vv{\tau^0(\alpha)}\\
\vdots\\
\vv{\tau^s(\alpha)}
\end{array}
\right)
 $$
 where the left matrix is of the size $\sqrt{mn} \times \sqrt{mn}$ and the right one is of the size $\sqrt{mn} \times mn$. Thus the 
 multiplication can be done with cost $O((mn)^{1/2 \omega(2)})$. Note that the total number of $T_{ij}$ is $n\cdot\dfrac{m}{s} = 
 \sqrt{mn}$. So using the similar idea to the abelian case we can apply powers of $\tau^s$ to them, so that we get 
 $T_0, \ldots, T_n$ and the cost is $\tilde{O}((mn)^{3/4\omega(4/3)})$.
 
 The final step is to compute $$\sigma^0(T_0), \sigma^1(T_1) , \ldots , \sigma^n (T_n).$$
Note that since $n \leq \sqrt{mn}$, this computation can be done using $\thecost$ operations in $F$, using the same ideas from the abelian case. Now we have proved the following theorem.

\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is a metacyclic group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

\section{Testing Invertiblity of an Element of the Group Algebra}



\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography} 

\end{document}
