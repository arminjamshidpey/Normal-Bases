\section{Computing projections of the orbit sum}
\label{sec:osum}

In this section we present an algorithm to compute $s_{\alpha,\ell}$,
when $G$ is polycyclic. We briefly present a definition of polycyclic 
groups and well known results on them while we refer the reader to
see ~\cite[Chapter 8]{HoEiOb05} for more details.

A group $G$ is called polycyclic if it has a normal series 
$$G = G_1 \unrhd G_2 \unrhd \cdots \unrhd G_n \unrhd G_{n+1} = 1,$$
where $G_j/G_{j+1}$ is cyclic. Finitely generated nilpotent or 
abelian groups are polycyclic. In general any finite solvable group is 
polycyclic.

For our purposes, the most interesting property of a finite polycyclic 
group is the fact that each element can be written as 
$$g_1^{i_1} \cdots g_n^{i_n}, 1 \leq j \leq n, 0 \leq i_j < r_j,$$
where $G_j/G_{j+1} = \langle g_jG_{j+1} \rangle$ and 
$\vert G_j/G_{j+1}\vert = r_j$.

Since abelian and metacyclic groups are particular cases that we focus on,
it is worth to take a closer look at presentations of these families.
We assume an abelian group $G$ is presented as 
\[ \label{eq:abeliangrp}
  \langle g_1, \ldots , g_r: g_{1}^{e_1} = \cdots = g_{r}^{e_r} = 1
  \rangle,
\]
where $ e_i \in \mathbb{N}$ is the order of $g_i$ and $n = e_1 \cdots
e_r$.  Without loss of generality, we assume $e_i \ge 2$ for all $i$,
so that $r$ is in $O(\log n)$. Elements of $\F[G]$ are written as
polynomials $\sum_{i_1,\dots,i_r} c_{i_1,\dots,i_r}
{g_1}^{e_1} \cdots {g_r}^{e_r}$, with $0\le i_j < e_j$ for all~$j$.

We start by sketching our
ideas in simplest case, cyclic groups.  We will see that they follow
closely ideas used by \cite{KalSho98} over finite fields.

Suppose $G = \langle g \rangle$, so that given $\alpha$ in $\K$ and
$\ell: \K \to \F$, our goal is to compute
\begin{equation}
  \label{eq:cycproj}
  \ell(g^i(\alpha)), ~~\mbox{for}~ 0\leq i\leq n-1.
\end{equation}
\cite{KalSho98} call this the \emph{automorphism projection problem} and
gave an algorithm to solve it in subquadratic time, when $g$ is the
$q$-power Frobenius $\mathbb{F}_{q^n} \to \mathbb{F}_{q^n}$.  The key idea in their
algorithm is to use the baby-steps/giant-steps technique: for a suitable
parameter $t$, the values in \eqref{eq:cycproj} can be rewritten as
\[
  (\ell \circ g^{tj})(g^i(\alpha)), ~~\mbox{for}~ 0 \leq j < m:=\lceil n/t
  \rceil ~\mbox{and}~ 0 \leq i <t.
\]
First, we compute all $G_i:=g^i(\alpha)$ for $0 \leq i <t$.  Then we compute
all $L_j:=\ell \circ g^{tj}$ for $0 \leq j <m$, where the $L_j$'s are
themselves linear mappings $\K \to \F$.  Finally, a matrix product yields
all values $L_j(G_i)$.

The original algorithm of \cite{KalSho98} relies on the properties of
the Frobenius mapping to achieve subquadratic runtime. In our case, we
cannot apply these results directly; instead, we have to revisit the
proofs of~\cite[Lemmata 3 and 4]{KalSho98}, now considering
rectangular matrix multiplication.  Our exponents involve the constant
$\omega(4/3)$, for which we have the upper bound $\omega(4/3) <
2.654$: this follows from the upper bounds on $\omega(1.3)$ and
$\omega(1.4)$ given by~\cite{LeGall}, and the fact that $k \mapsto
\omega(k)$ is convex~\citep{LoRo83}. In particular, $3/4 \cdot
\omega(4/3) < 1.99$. Note also the inequality $\omega(k) \ge 1+k$ for
$k\ge 1$, since $\omega(k)$ describes products with input and output
size $O(n^{1+k})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multiple automorphism evaluation and applications}

The key to the algorithms below is the remark following
Assumption~\ref{assum}, which reduces automorphism evaluation to
modular composition of polynomials.  Over finite fields, this idea goes back
to~\cite{GaSh92}, where it was credited to Kaltofen.

For instance, given $g \in G$ (by means of $\gamma:=g(\xbar)$), we can
deduce $g^2 \in G$ (again, by means of its image at $\xbar$) as
$\gamma(\gamma)$; this can be done with $\tilde{O}(n^{(\omega+1)/2})$
operations in $\F$ using Brent and Kung's modular composition
algorithm~\citep{BrKu78}. The algorithms below describe similar operations
along these lines, involving several simultaneous evaluations.

\begin{lemma}
  \label{lem:modcom}
  Given $\alpha_1,\dots,\alpha_s$ in $\K$ and $g$ in $G =
  \mathrm{Gal}(\K/\F)$, with $s = O(\sqrt{n})$, we can compute
  $g(\alpha_1),\dots,g(\alpha_s)$ with $\tilde
  O(n^{(3/4)\cdot\omega(4/3)})$ operations in $\F$.
\end{lemma}
\begin{proof}
(Compare \cite[Lemma~3]{KalSho98}) As noted above, for $i\le s$,
  $g(\alpha_i) = \alpha_i(\gamma)$, with $\gamma := g(\xbar) \in \K$.
  Let $t := \lceil n^{3/4} \rceil$, $m:=\lceil n/t\rceil$, and rewrite $\alpha_1 , \ldots , \alpha_s$ as 
$$\alpha_i = \sum_{0 \leq j < m} a_{i,j}\xbar^{tj},$$ where the
  $a_{i,j}$'s are polynomials of degree less than $t$. The next step
  is to compute $\gamma_i := \gamma^i$, for $i = 0 , \ldots , t$.
  There are $t$ products in $\K$ to perform, so this amounts to
  $\tilde{O}(n^{7/4})$ operations in $\F$.

  Having $\gamma_i$'s in hand, one can form the matrix
  $\boldsymbol{\Gamma} := \left[ \Gamma_0 ~ \cdots ~ \Gamma_{t-1}
    \right]^T$, where each column $\Gamma_i$ is the coefficient vector
  of $\gamma_i$ (with entries in $\F$); this matrix has $t \in
  O(n^{3/4})$ rows and $n$ columns. We also form
  $$\mat A := \left[{A}_{1,0} \cdots {A}_{1,m-1} \cdots
    {A}_{s,0} \cdots {A}_{s,m-1}\right]^T,$$ where
  ${A}_{i,j}$ is the coefficient vector of $a_{i,j}$. This matrix 
  has $s m \in O(n^{3/4})$ rows and $t \in O(n^{3/4})$ columns.

  Compute $\mat B:=\mat A\, \boldsymbol{\Gamma}$; as per our
  definition of exponents $\omega(\cdot )$, this can be done in
  $O(n^{(3/4)\cdot \omega(4/3)})$ operations in $\F$, and the rows of this matrix
  give all $a_{i,j}(\gamma)$.  The last step to get all
  $\alpha_i(\gamma)$ is to write them as $\alpha_i(\gamma) = \sum_{0
    \leq j < m} a_{i,j}(\gamma) \gamma_t^{j}.$ Using Horner's scheme,
  this takes $O(sm)$ operations in $\K$, which is $\tilde{O}(n^{7/4})$
  operations in $\F$. Since we pointed out that $\omega(3/4) \ge 7/4$,
  the leading exponent in all costs seen so far is
  $(3/4)\cdot\omega(4/3)$.
\end{proof}


\begin{lemma}\label{lem:selfcomp}
  Consider $g_1, \ldots g_r$ in $G=\mathrm{Gal}(\K/\F)$, positive
  integers $(s_1, \ldots s_r)$ and elements $\alpha_{i_1, \dots, i_r}$
  in $\K$, for $i_m=0,\dots,s_m$, $m=1,\dots,r$. If $\prod_{i = 1}^r s_i
  = O(\sqrt{n})$, we can compute
  $$g_r^{i_r}\cdots g_1^{i_1}(\alpha_{i_1, \dots, i_r})
  \text{~for~} i_m=0,\dots,s_m,\ m=1,\dots,r
  $$  using $\thecost$ operations in $\F$.
\end{lemma}
\begin{proof}
  Define $\mathcal I = \{(i_1,\dots,i_r) \mid i_m=0,\dots,s_m
  \text{~for~} m=1,\dots,r\}$. For $(i_1,\dots,i_r)$ in $\mathcal I$
  and non-negative integers $\ell_1,\dots,\ell_r$, define
  $$\alpha_{i_1, \dots, i_r}^{(\ell_1,\dots,\ell_r)} 
  =g_r^{\ell_r} \cdots g_1^{\ell_1}(\alpha_{i_1, \dots, i_r}).$$  
  Assume then that
  for some $t$ in $\{0,\dots,r-1\}$, we know 
  $$S_t=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},0, \dots, 0)} \ \mid \ (i_1,\dots,i_r)\in \mathcal I);$$ we show how to compute 
  $$S_{t+1}=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t+1},0, \dots, 0)}
  \ \mid \ (i_1,\dots,i_r)\in \mathcal I).$$ Since our input is $S_0$,
  it will be enough to go through this process $r$ times to obtain the
  output $S_r$ of the algorithm.
  
  For a given index $t$, and for $m \ge 0$
  define further 
  $$S_{t,m}=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod 2^m, 0,\dots, 0)} \ \mid \ (i_1,\dots,i_r)\in \mathcal I);$$
  in particular, $S_{t,0} = S_t$ and $S_{t,\lfloor \log_2(s_{t+1})\rfloor+1} = S_{t+1}$. 
  Hence, given $S_{t,m}$, it is enough to show how to compute $S_{t,m+1}$,
  for indices $m=0,\dots,\lfloor \log_2(s_{t+1})\rfloor$.
  This is done by writing 
  $$S_{t,m+1}=
  (\beta_{i_1, \dots,i_r,t,m} \ \mid \ (i_1,\dots,i_r)\in \mathcal I),$$
  with
  $$\beta_{i_1, \dots,i_r,t,m}
  =\begin{cases}
  \alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod 2^m,0, \dots, 0)}  \text{~if~} 
  i_{t+1} \bmod 2^{m+1} = i_{t+1} \bmod 2^m \\[2mm]
  g_{t+1}^{2^{m}}(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod 2^m,0, \dots, 0)}) 
  \text{~otherwise.}
  \end{cases}$$
  The automorphisms $g_{t+1}^{2^m}$ can be computed iteratively by modular
  composition; the bottleneck is the application of  $g_{t+1}^{2^m}$
  to a subset of $S_{t,m}$. Using Lemma \ref{lem:modcom}, since 
  $S_{t,m}$ has $O(\sqrt n)$ elements, this take $\thecost$ 
  operations in $\F$.
  
  For a given index $t$, this is repeated $O(\log(s_{t+1}))$ times;
  adding up for all such indices, this amounts to $O(\log (s_1 \cdots s_r))$,
  which is $O(\log(n))$; the conclusion follows.
\end{proof}


%% \begin{lemma}\label{lem:selfcomp}
%% Given $\alpha$ in $\K$, $g_1, \ldots , g_{r}$ in $G =
%% \mathrm{Gal}(\K/\F)$ and positive integers $(s_1, \ldots s_r)$ such
%% that $\prod_{i = 1}^r s_i = O(\sqrt{n})$ and $r \in O(\log(n))$, all
%%   $$g_1^{i_1}\cdots g_r^{i_r}(\alpha) ,\quad \text{~for~} 0 \leq i_j
%% \leq s_j,\ 1 \leq j \leq r$$ can be computed in $\osumcosttilde$
%% operations in $\F$.
%% \end{lemma}
%% \begin{proof}
%% (Compare \cite[Lemma~4]{KalSho98}) For a given  $m\in\{1,\dots,r\}$, suppose we have computed 
%%   $$G_{i_1,\dots,i_m}:=g_m^{i_m}\cdots g_1^{i_1}(\alpha)$$ for $0 \leq
%%   i_j \leq s_j$ if $1 \leq j < m$, and $0 \leq i_m < k_m,$ as well as
%%   the automorphism $\eta:={g_m}^{k_m}$ (by means of its value at $\xbar$, as per our convention).
  
%%  Then, we can obtain $G_{i_1,\dots,i_m}$ for $0 \leq i_j \leq s_j$ if $1
%%  \leq j < m$, and $0 \leq i_m < 2k_m$, by computing
%%  $\eta(G_{i_1,\dots,i_m})$, for all indices $i_1,\dots,i_m$ available
%%  to us, that is, $0 \leq i_j \leq s_j$ if $1 \leq j < m$, and $0 \leq
%%  i_m < k_m$. This can be carried out using $\osumcosttilde$ operations
%%  in $\F$ by applying Lemma \ref{lem:modcom}. Prior to entering the
%%  next iteration, we also compute $\eta^2$ by means of one modular
%%  composition, whose cost is negligible. 

%%  Using the above doubling method for $g_m$, we have to do $O(\log
%%  s_m)$ steps, for a total cost of $\osumcosttilde$ operations in $\F$.  We
%%  repeat this procedure for $m=1,\dots,r$; since $r$ is in $O(\log(n))$,
%%  the cost remains $\osumcosttilde$.
%% \end{proof}

We now present dual versions of the previous two lemmas
(\cite{KalSho98} also have such a discussion). Seen as an $\F$-linear
map, the operator $g:\alpha \mapsto g(\alpha)$ admits a transpose,
which maps an $\F$-linear form $\ell:\K\to\F$ to the $\F$-linear form
$\ell \circ g: \alpha \mapsto \ell(g(\alpha))$.  The {\em
  transposition principle}~\citep{KaKiBs88,CaKaYa89} implies that if a
linear map $\F^N \to \F^M$ can be computed in time $T$, its transpose
can be computed in time $T+O(N+M)$. In particular, given $s$ linear
forms $\ell_1,\dots,\ell_s$ and $g$ in $G$, transposing
Lemma~\ref{lem:modcom} shows that we can compute $\ell_1 \circ
g,\dots,\ell_s \circ g$ in time $\osumcosttilde$. The following lemma
sketches the construction.

\begin{lemma}
  \label{lem:modcomT}
  Given $\F$-linear forms $\ell_1,\dots,\ell_s:\K\to \F$ and $g$ in $G =
  \mathrm{Gal}(\K/\F)$, with $s = O(\sqrt{n})$, we can compute
  $\ell_1\circ g,\dots,\ell_s \circ g$ in time $\tilde
  O(n^{{3}/{4}\omega({4}/{3})})$.
\end{lemma}
\begin{proof}
  Given $\ell_i$ by its values on the power basis $1,\xbar,\dots,\xbar^{n-1}$, $\ell_i \circ g$ is represented by its values at
  $1,\gamma,\dots,\gamma^{n-1}$, with $\gamma := g(\xbar)$. 

  Let $t,m$ and $\gamma_0,\dots,\gamma_t$ be as in the proof of
  Lemma~\ref{lem:modcom}. Next, compute the ``giant steps''
  $\gamma_t^j = \gamma^{tj}$, $j=0,\dots,m-1$ and for $i=1,\dots,s$
  and $j=0,\dots,m-1$, deduce the linear forms $L_{i,j}$ defined by
  $L_{i,j}(\alpha) := \ell_i(\gamma^{tj}\alpha)$ for all $\alpha$ in
  $\K$. Each of them can be obtained by a {\em transposed
    multiplication} in time $\tilde{O}(n)$~\citep[Section~4.1]{Shoup},
  so that the total cost thus far is $\tilde{O}(n^{7/4})$.

  Finally, multiply the $(sm \times n)$ matrix with entries the
  coefficients of all $L_{i,j}$ (as rows) by the $(n \times t)$ matrix with
  entries the coefficients of $\gamma_0,\dots,\gamma_{t-1}$ (as columns) to
  obtain all values $\ell_i(\gamma^j)$, for $i=1,\dots,s$ an
  $j=0,\dots,n-1$.  This can be accomplished with
  $O(n^{(3/4)\cdot\omega(4/3)})$ operations in~$\F$.
\end{proof}

From this, we deduce the transposed version of Lemma~\ref{lem:selfcomp},
whose proof follows the same pattern.

\begin{lemma}
  \label{lem:transmodcomp}
  Consider $g_1, \ldots g_r$ in $G=\mathrm{Gal}(\K/\F)$, positive
  integers $(s_1, \ldots s_r)$ and $\F$-linear forms $\ell_{i_1,
    \dots, i_r}$, for $i_m=0,\dots,s_m$, $m=1,\dots,r$. If $\prod_{i =
    1}^r s_i = O(\sqrt{n})$, we can compute
  $$\ell_{i_1, \dots, i_r} \circ g_r^{i_r}\cdots g_1^{i_1}
  \text{~for~} i_m=0,\dots,s_m,\ m=1,\dots,r
  $$  using $\thecost$ operations in $\F$.
  %% Given $\ell:\K\to F$, $g_1, \ldots , g_{r}$ in $G = \mathrm{Gal}(\K/\F)$
  %% and positive integers $(s_1, \ldots s_r)$ such that
  %% $\prod_{i = 1}^r s_i = O(\sqrt{n})$ and $r \in O(\log(n))$, all linear
  %% maps
  %% \[
  %%   \ell \circ g_1^{i_1}\cdots g_r^{i_r} ,\quad \text{~for~} 0 \leq i_j
  %%   \leq s_j,\ 1 \leq j \leq r,
  %% \]
  %% can be computed in $\osumcosttilde$ operations in $\F$.
\end{lemma} 
\begin{proof}
  We proceed as in Lemma~\ref{lem:selfcomp}, reversing the order of
  the steps. Using the same index set $\mathcal I$ as before, define,
  for $(i_1,\dots,i_r)$ in $\mathcal I$ and non-negative integers
  $k_1,\dots,k_r$
  $$\ell_{i_1,\dots,i_r}^{(k_1,\dots,k_r)} =\ell_{i_1,\dots,i_r} \circ g_r^{k_r}\cdots g_1^{k_1}.$$
  For $t=r,\dots,0$, assuming that
  we know 
  $$L_{t+1} = (\ell_{i_1, \dots,i_r}^{(0, \dots, 0,i_{t+1},\dots,i_r)} \ \mid
  \ (i_1,\dots,i_r)\in \mathcal I),$$ we compute 
  $$L_{t}=(\ell_{i_1, \dots,i_r}^{(0, \dots, 0,i_{t},i_{t+1},\dots,i_r)}
  \ \mid \ (i_1,\dots,i_r)\in \mathcal I).$$
  This time, for $m \ge 0$, we set
  $$L_{t+1,m} = (\ell_{i_1, \dots,i_r}^{(0, \dots, 0,\lfloor i_{t}
    \rfloor_m,i_{t+1},\dots,i_r)} \ \mid \ (i_1,\dots,i_r)\in \mathcal
  I),$$ where for a non-negative integer $x$, $\lfloor x \rfloor_m = x
  - (x \bmod (2^m-1))$ is obtained by setting to zero the coefficients
  of $1,2,\dots,2^{m-1}$ in the base-two expansion of $x$.

  Starting from $L_{t+1} = L_{t, \lceil \log_2(s_t) \rceil +1}$, we
  compute all $L_{t+1,m}$ for $m= \lceil \log_2(s_t) \rceil,\dots,0$,
  since $L_{t+1,0} = L_{t}$. This is done essentially as in
  Lemma~\ref{lem:selfcomp}, but using Lemma~\ref{lem:modcomT} this
  time, in order to do right-composition by $g_t^{2^m}$.
  The cost analysis is as in Lemma~\ref{lem:selfcomp}.
\end{proof}

%% At this point we have enough tools to see how the computation is
%% done in the cyclic case. Moreover, we can use the above lemmas to
%% give an algorithm for a more general case, namely the abelian
%% case. With a little bit more work we can state an algorithm which
%% solves the automorphism projection problem when $G = \lbrace a^ib^j
%% \rbrace$ where $m \leq n$. As an specific case this solves the
%% problem for metacyclic groups.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Abelian Groups}
\label{ssec:proj_abelian}

The first main result in this section is the following proposition.
Assume $G$ is an abelian group presented as 
\[ \label{eq:abeliangrp}
  \langle g_1, \ldots , g_r: g_{1}^{e_1} = \cdots = g_{r}^{e_r} = 1
  \rangle,
\]
where $ e_i \in \mathbb{N}$ is the order of $g_i$ and $n = e_1 \cdots
e_r$.  Without loss of generality, we assume $e_i \ge 2$ for all $i$,
so that $r$ is in $O(\log n)$. Elements of $\F[G]$ are written as
polynomials $\sum_{i_1,\dots,i_r} c_{i_1,\dots,i_r}
{g_1}^{e_1} \cdots {g_r}^{e_r}$, with $0\le i_j < e_j$ for all~$j$.


\begin{proposition}\label{prop:abelian}
  Suppose that $G$ is abelian, with notation as above. For $\alpha$ in
  $\K$ and $\ell:\K\to\F$, $s_{\alpha,\ell} \in \F[G]$, as defined
  in~\eqref{def:s_alpha_ell}, is computable using $\osumcosttilde$
  operations in $\F$.
\end{proposition}
\begin{proof}
Our goal is to compute
\begin{equation}\label{eq:abelian}
  \ell (g_1^{i_1},  \ldots, g_r^{i_r}(\alpha)), \, 1 \leq j \leq r, 0 \leq i_j < e_j,
\end{equation}
where $\ell$ is an $\F$-linear projection $\K\to \F$.  For $ 1\leq i
\leq r$ and (appropriate) $1 \leq s_i, t_i \leq e_i$, as we sketched in the
cyclic case, the elements in \eqref{eq:abelian} can be expressed as
$L_{j_1,\dots, j_r} (G_{i_1,\dots,i_r})$, 
for $1\leq m \leq r, 0\leq i_m < s_m, 0 \leq j_m < t_m$.
Here, $L_{j_1,\dots,j_r} :=\ell \circ (g_1^{j_1} \cdots
g_r^{j_s})$ are linear projections presented as row vectors and
$G_{i_1,\dots,i_r}:=g_1^{i_1} \cdots g_r^{i_r}(\alpha)$ are field
elements presented as column vectors. Define $z$ to be the unique index such that 
$\prod_{i = 1}^{z} e_i < \lceil \sqrt{n} \rceil$ and 
$\prod_{i = 1}^{z+1} e_i \geq \lceil \sqrt{n} \rceil.$ 
%Moreover, let 
%\begin{equation*}
%\begin{array}{cc}
%s_i = \begin{cases}
% e_i & 1 \leq i \leq z \\
% \lceil \frac{\sqrt{n}}{e_1 \cdots e_z} \rceil & i = z+1\\
% 1 & z+2 \leq i \leq r
%\end{cases} & t_i = \begin{cases}
% 1 & 1 \leq i \leq z \\
% \lceil \frac{e_{z+1}}{s_i} \rceil & i = z+1\\
% e_i & z+2 \leq i \leq r
%\end{cases} 
%\end{array}
%\end{equation*}
Then, all elements in
\eqref{eq:abelian} can be computed with the following steps, the sum of whose 
costs proves the proposition.

\smallskip\noindent \textbf{Step 1.} Apply Lemma \ref{lem:selfcomp},
with $\alpha_{i_1,\dots,i_r} = \alpha$ for all $i_1,\dots,i_r$, to get
$$
G_{i_1,\dots,i_r}=g_1^{i_1} \cdots g_z^{i_z}(\alpha),$$
$$
 1\leq m \leq z-1, 0\leq i_m < e_m,
0\leq i_z < \Big\lceil \frac{\sqrt{n}}{e_1 \cdots e_z} \Big\rceil,
$$
with cost $\osumcosttilde$.

\smallskip\noindent\textbf{Step 2.} Compute all $g_z^{s_z}$.

\smallskip\noindent\textbf{Step 3.} Use Lemma \ref{lem:transmodcomp}
with $\ell_{i_1,\dots,i_r} = \ell$ for all $i_1,\dots,i_r$, to compute 
$$L_{j_1,\dots,j_r} = \ell \circ (g_{z}^{s_z j_z}
g_{z+1}^{j_{z+1}}\cdots g_r^{j_s}),$$
$$
0 \leq j_{z} <  \Big\lceil \frac{e_{z+1}}{s_i} \Big\rceil , z+1\leq m \leq r, 0 \leq j_m < e_m,$$
with cost $\osumcosttilde$

\smallskip\noindent\textbf{Step 4.} Multiply the matrix with rows the
coefficients of all $L_{j_1,\dots,j_r}$ by the matrix with columns the
coefficients of all $G_{i_1,\dots,i_r}$; this yields all required
values. We compute this product in $O(n^{(1/2)\cdot\omega(2)})$
operations in $\F$, which is in $\osumcost$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Metacyclic Groups}

A group $G$ is metacyclic if it has a normal cyclic subgroup $H$ such that
$G/H$ is cyclic; for instance, any group with a squarefree order is
metacyclic. See \citep[p.~88]{Johnson} or \citep[p.~334]{Curtis} for more
background. A metacyclic group can always be presented~as
\begin{equation}
  \label{eq:metacyclic}
  \langle \sigma,\tau: \sigma^m = 1,  \tau^s = \sigma^t, \tau^{-1}\sigma \tau = \sigma^r \rangle,
\end{equation}
for some integers $m,t,r,s$, with $r,t \leq m$ and
$r^s = 1 \bmod t, rt = t \bmod m$. For example, the dihedral group
$$D_{2m} = \langle \sigma,\tau: \sigma^m =1, \tau^2 = 1, \tau^{-1}
\sigma \tau = \sigma^{m-1} \rangle, $$ is metacyclic, with
$s=2$. Generalized quaternion groups, which can be presented as
$$Q_m = \langle \sigma,\tau: \sigma^{2m} =1, \tau^2 = \sigma^m,
\tau^{-1} \sigma \tau = \sigma^{2m-1} \rangle,$$ are metacyclic, with
$s=2$ as well.

Using the notation of~\eqref{eq:metacyclic}, $n=|G|$ is equal to $ms$, and
all elements in a metacyclic group can be presented uniquely as either
\begin{equation}\label{pres1}
\{\sigma^i \tau^j,\,\,\, 0\leq i \leq m-1,\ 0\leq j \leq s-1\}  
\end{equation}
or
\begin{equation}\label{pres2}
\{ \tau^j\sigma^i,\,\,\, 0\leq i \leq m-1,\ 0\leq j \leq s-1\}.
\end{equation}
Accordingly, elements in the group algebra $\F[G]$ can be written as 
either 
$$\sum_{\substack{i <m\\ j< s}} c_{i,j} \sigma^i \tau^j \quad\text{or}\quad
\sum_{\substack{i <m\\ j< s}} c'_{i,j} \tau^j \sigma^i.$$
Conversion between the two representations involves no operation in $\F$,
using the commutation relation $\sigma^k \tau^c = \tau^c \sigma^{kr^c}$
for $k,c \ge 0$.

\begin{proposition}\label{prop:sum_meta}
  Suppose that $G$ is metacyclic, with notation as above. For $\alpha$
  in $\K$ and $\ell:\K\to\F$, $s_{\alpha,\ell} \in \F[G]$ is
  computable using $\osumcosttilde$ operations in $\F$.
\end{proposition}
\begin{proof}
  Suppose first that $s \le m$; then, we use the
  presentation~\eqref{pres1} of the elements of $G$. Take $\alpha$ in
  $\K$ and $\ell:\K \to \F$; the goal is to compute
  $\ell(\sigma^i\tau^j (\alpha))$, for all $0\leq i < m$ and $0 \leq j
  <s.$ This is accomplished with the following steps.

\smallskip\noindent\textbf{Step 1.} Apply Lemma \ref{lem:selfcomp},
with $\alpha_{i,j}=\alpha$ for all $i,j$, to compute
$$G_{i,j} := \sigma^i\tau^j(\alpha),\ 0\leq i < \lceil \sqrt{m/s}
\rceil,\ 0 \leq j < s.$$ Note that
$\lceil \sqrt{m/s} \rceil s \leq \lceil \sqrt{sm} \rceil \in O(\sqrt n)$,
so we are under the assumptions of the lemma. This takes $\osumcosttilde$
operations in~$\F$.

\smallskip\noindent\textbf{Step 2.} Compute
$\sigma^{\lceil \sqrt{m/s} \rceil}$, in $O(\log(n))$ modular compositions
in degree $n$. The cost is no more than that of Step 1.

\smallskip\noindent\textbf{Step 3.} Compute
\[
  L_k := \ell \circ \sigma^{k\lceil \sqrt{m/s} \rceil}, \,\, 0\leq k <
  \lceil \sqrt{sm}\rceil,
\]
using Lemma \ref{lem:transmodcomp} (with $\ell_{i,j}=\ell$ for all
$i,j$).  This takes $\osumcosttilde$ operations in~$\F$.

\smallskip\noindent\textbf{Step 4.} At this point, we compute all
$$ L_k(s_{i,j}) = \ell(\sigma^{k\lceil \sqrt{m/s} \rceil + i}\tau
^j(\alpha)),$$ for $0\le k < \lceil \sqrt{sm}\rceil$,
$0\le i< \lceil \sqrt{m/s}\rceil$ and $0 \leq j < s;$ these are precisely
the values we needed.

This can be carried out by multiplying the matrix with rows the
coefficients of all $L_k$ by the matrix with columns the coefficients of
all $G_{i,j}$; this yields all required values, as pointed out above. There
are $O(\sqrt{sm})=O(\sqrt{n})$ linear forms $L_k$'s, and $O(\sqrt{n})$
field elements $G_{i,j}$'s, so we can compute this product in
$O(n^{(1/2)\cdot\omega(2)})$ operations in $\F$, which is $\osumcost$.

This concludes the proof in the case $s \le m$. When $m \le s$, use the
presentation~\eqref{pres2} of the elements of $G$ and proceed as above.
\end{proof}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NormalBasisCharZero"
%%% End:
